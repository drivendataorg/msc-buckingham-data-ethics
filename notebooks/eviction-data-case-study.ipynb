{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Ethics Checklist - University of Buckingham MSc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goals of this notebook exercise are (1) to practice integrating the `deon` checklist into your code, and (2) to learn how to use a few basic data science tools in python.\n",
    "\n",
    "This notebook is for the Eviction Data Case Study exercise in the \"Actionable Ethics for Data Scientists\" workshop for the data science MSc at the University of Buckingham.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- We'll walk through the notebook as a group, and break for independent work where there is an <span style=\"color:green\">***italicized, bolded heading for \"ACTIVITY\" or \"DISCUSSION\"***</span>.\n",
    "\n",
    "- If you need help debugging during any of the exercises, post in the chat or send a direct message to one of the DrivenData team members. **We encourage you to work together!**\n",
    "\n",
    "- There is a more comprehensive version of the case study notebook here: [https://github.com/drivendataorg/msc-buckingham-data-ethics/blob/master/notebooks/eviction-data-case-study-reference.ipynb](https://github.com/drivendataorg/msc-buckingham-data-ethics/blob/master/notebooks/eviction-data-case-study-reference.ipynb). You can refer to this if you are stumped during any of the coding exercises, but we strongly encourage solving problems on your own first!\n",
    "\n",
    "Notebook outline:\n",
    "\n",
    "- Background\n",
    "- Set up Python\n",
    "- Load & explore the data\n",
    "- Train a model\n",
    "- Walk through `deon` checklist\n",
    "    - Activities & discussion\n",
    "\n",
    "To easily jump beween notebook sections in Google colab, open the outline sidebar by clicking the three horizontal lines in the side menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past five decades in the US, [housing costs have risen faster than incomes](http://www.jchs.harvard.edu/state-nations-housing-2018), low-cost housing has been disappearing from the market, and racial disparities in homeownership rates have deepened. This has put many in a perilous situation. As the [Eviction Lab](https://evictionlab.org/why-eviction-matters/#affordable-housing-crisis) explains:\n",
    "\n",
    "> Today, most poor renting families spend at least half of their income on housing costs, with one in four of those families spending over 70 percent of their income just on rent and utilities. Only one in four families who qualify for affordable housing programs get any kind of help. Under those conditions, it has become harder for low-income families to keep up with rent and utility costs, and a growing number are living one misstep or emergency away from eviction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "A non-profit dedicated to helping people at risk of eviction in California has tasked us to build a model to estimate the number of eviction cases by geography, based on socioeconomic data. They would like to use these estimates to help them prioritize where to commit funding and resources.\n",
    "\n",
    "We will be using a subset of the eviction dataset published by the [Eviction Lab](https://evictionlab.org/) at Princeton University. The subset is the census-tract-level aggregates for only tracts in the state of California. \n",
    "\n",
    "*FYI:* [Census tracts](https://www.census.gov/programs-surveys/geography/about/glossary.html#:~:text=Census%20tracts%20generally%20have%20a,on%20the%20density%20of%20settlement.) are small, relatively permanent geographic areas used in the U.S. census. A tract generally has a population between 1,200 and 8,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Set up Python\n",
    "\n",
    "Install and import the necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell below if you are working in Google colab\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import wget\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from where we have it saved online\n",
    "DATA_URL = \"https://drivendata-public-assets.s3.amazonaws.com/odsc-west-2019/california-tracts.csv\"\n",
    "DATA_PATH = Path(\"../data/raw/california-tracts.csv\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    DATA_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "    # Download data\n",
    "    wget.download(url=DATA_URL, out=str(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "\n",
    "Let's do some basic exploration of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data using pandas\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = non-NaN observations; size = all observations\n",
    "df.groupby(\"year\").agg(\n",
    "    count=(\"eviction-rate\", \"count\"), size=(\"eviction-rate\", \"size\")\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dictionary\n",
    "DATA_DICT_URL = \"https://drivendata-public-assets.s3.amazonaws.com/odsc-west-2019/DATA_DICTIONARY.txt\"\n",
    "DATA_DICT_PATH = Path(\"../references/DATA_DICTIONARY.txt\")\n",
    "\n",
    "if not DATA_DICT_PATH.exists():\n",
    "    DATA_DICT_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "    # Download data dictionary\n",
    "    wget.download(url=DATA_DICT_URL, out=str(DATA_DICT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what information do we have in the dataset?\n",
    "!cat $DATA_DICT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "The non-profit wants their decision-making to be race-blind, so they ask for the population race percentage features to not be included in the modeling.\n",
    "\n",
    "We'll create a very basic model that predicts evictions. We can refer to this model as we consider the ethics checklist items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the range of eviction values?\n",
    "df.evictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = \"evictions\"\n",
    "FEATURE_VARS = [\n",
    "    \"year\",\n",
    "    \"population\",\n",
    "    \"poverty-rate\",\n",
    "    \"median-property-value\",\n",
    "    \"renter-occupied-households\",\n",
    "    \"pct-renter-occupied\",\n",
    "    \"median-gross-rent\",\n",
    "    \"median-household-income\",\n",
    "    \"rent-burden\",\n",
    "    ## Don't include race features\n",
    "    #'pct-white' , 'pct-af-am', 'pct-hispanic', 'pct-am-ind',\n",
    "    #'pct-asian', 'pct-nh-pi', 'pct-multiple', 'pct-other'\n",
    "    ## Also don't include features directly related to the target variable\n",
    "    # 'eviction-filings', 'eviction-rate', 'eviction-filing-rate'\n",
    "]\n",
    "GROUP_VAR = \"GEOID\"  # Prevent leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Shape: {df.shape}\")\n",
    "# Drop NAs in target variable\n",
    "df_modeling = df.dropna(subset=[TARGET_VAR]).copy()\n",
    "df_modeling.reset_index(inplace=True)\n",
    "print(f\"Shape without NAs: {df_modeling.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, cross_validate\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train-test split for model evaluation later\n",
    "split = GroupShuffleSplit(test_size=0.20, n_splits=2, random_state=36).split(\n",
    "    df_modeling, groups=df_modeling.loc[:, GROUP_VAR]\n",
    ")\n",
    "\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "df_train = df_modeling.loc[train_inds, :]\n",
    "df_test = df_modeling.loc[test_inds, :]\n",
    "\n",
    "X_train = df_train.loc[:, FEATURE_VARS].values\n",
    "y_train = df_train.loc[:, TARGET_VAR].values\n",
    "\n",
    "X_test = df_test.loc[:, FEATURE_VARS].values\n",
    "y_test = df_test.loc[:, TARGET_VAR].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to fit and use a [random forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) model to predict evictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model - note that this may take a few moments\n",
    "model_pipeline = Pipeline([\n",
    "    ('med_impute', SimpleImputer(strategy='median')),\n",
    "    ('model', RandomForestRegressor(\n",
    "        criterion='friedman_mse',\n",
    "        n_estimators=100, \n",
    "        max_depth=10,\n",
    "        random_state=36\n",
    "    ))\n",
    "])\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions and look at key performance metrics\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "print(\"R2\", r2_score(y_test, y_pred))\n",
    "print(\"MSE\", mean_squared_error(y_test, y_pred))\n",
    "print(\"MAE\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic visualization of actual v predicted\n",
    "_, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(y_test, y_pred, \".\", markersize=2, alpha=0.2)\n",
    "plt.xlabel(\"Actual evictions\")\n",
    "plt.ylabel(\"Predicted evictions\")\n",
    "\n",
    "# Set aspect to square so it's easier to see correlation\n",
    "plt.xlim([0, 200])\n",
    "plt.ylim([0, 200])\n",
    "ax.axline([0, 0], slope=1, color=\"black\", linewidth=1, label=\"Predicted = Actual\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a little hard to see the pattern because of the number of data points we have. For now, we can see that most values are clustered at low numbers of evictions. When there is a high number of evictions (more than ~50), our model has a tendency to underpredict the number of evictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Walk through `deon` Checklist\n",
    "\n",
    "We'll now go through a few of the items in `deon`'s standard ethics checklist. \n",
    "\n",
    "**We will not discuss every item on the deon checklist - items have been chosen that illustrate interesting points or coding challenges.** In real-world setting, you'll want to integrate the full deon checklist into your coding. You can see an [example](https://github.com/drivendataorg/msc-buckingham-data-ethics/blob/master/notebooks/eviction-data-case-study-reference.ipynb) of this in the workshop repository.\n",
    "\n",
    "In the future, you can create your own ethics checklist walkthrough notebooks easily with `deon --output ethics-checklist.ipynb`. See the `deon` [documentation](https://deon.drivendata.org/#command-line-options) for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains example code to considering possible sources of bias in the data. This code may be a helpful reference when you are completing exercises independently later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the number of missing values for a handful of relevant columns\n",
    "df[[\"low-flag\", \"imputed\", \"evictions\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations have the low-flag?\n",
    "# A majority of the evictions are likely too low\n",
    "print(f\"Proportion of observations with low-flag: {df['low-flag'].mean():.2f}\")\n",
    "df[\"low-flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations have the imputed flag?\n",
    "# Very few values for eviction were imputed\n",
    "print(f\"Proportion of observations with imputed flag: {df['imputed'].mean():.2f}\")\n",
    "df[\"imputed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the general values of the race columns?\n",
    "race_cols = [\n",
    "    \"pct-white\",\n",
    "    \"pct-af-am\",\n",
    "    \"pct-hispanic\",\n",
    "    \"pct-am-ind\",\n",
    "    \"pct-asian\",\n",
    "    \"pct-nh-pi\",\n",
    "    \"pct-multiple\",\n",
    "    \"pct-other\",\n",
    "]\n",
    "df[race_cols].describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *One note on the data:* The history around defining racial categories is complex, flawed, and nuanced. For the purposes of this activity, we will accept the race-based categories in the data as is. In a real-world context, it would be worth discussing how to navigate these categories in the most equitable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise correlation of some columns against race percentage columns\n",
    "cols_to_correlate = [\"evictions\", \"median-household-income\", \"imputed\", \"low-flag\"]\n",
    "correlation_df = (\n",
    "    df[race_cols + cols_to_correlate].corr().loc[race_cols, cols_to_correlate]\n",
    ")\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the above correlations.\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.heatmap(\n",
    "    correlation_df.sort_values(\"evictions\"),\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    vmin=-0.5,\n",
    "    vmax=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly not applicable. We have no PII, but we do have some tracts with few observations. We need to be mindful of those and maybe exclude them from visualizations or combine them with neighboring tracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the non-profit's request, we did not include any race variables in training our model. We want to figure out whether the model is still making decisions based on race using proxy variables that can indirectly indicate race.\n",
    "\n",
    "**Any questions before we dive into our first activity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style=\"color:green\">*D.1 ACTIVITY*</span>\n",
    "\n",
    "> Work independently for 20-25 minutes. Start here and stop where \"end of activity\" is indicated. **We encourage you to collaborate with one another!**\n",
    "\n",
    "**<span style=\"color:green\">To what extent are any of the feature variables in our model acting as proxies for race? Take some time to explore the data.</span>**\n",
    "\n",
    "<span style=\"color:green\">First, let's look for correlations between our feature variables and our race variables.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_modeling.loc[:, FEATURE_VARS + race_cols].corr().loc[FEATURE_VARS, race_cols]\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu_r\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:green\">Takeaways</span>** \n",
    "\n",
    "- <span style=\"color:green\">*Example takeaway:* poverty rate has a strong correlation with multiple race variables. It tends to be higher for tracts with a higher percent hispanic, and also higher but slightly less so for tracts with a higher percent African American. It tends to be lower in neighborhoods that are more white.</span>\n",
    "\n",
    "- <span style=\"color:green\">... your thoughts here ...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">We can use the correlation function's documentation to help with interpretation (below). You may want to look online for more details about any concepts in the documentation that you aren't familiar with, like pearson correlation coefficients.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see documentation of the df.corr function\n",
    "?df.corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Your turn to code!</span>\n",
    "\n",
    "<span style=\"color:green\">We have a lot of different race variables, some of which have fairly low rates in many areas. **What happens if we create an aggregated variable for the percent of all non-white residents (`pct-non-white`)? What do the feature variable correlations look like for `pct-non-white` compared to `pct-white`, and do any patterns become clearer?** *Hint:* You can accomplish most of this by reusing code from above.</span>\n",
    "\n",
    "<span style=\"color:green\">Remember to document any substantive choices you have to make when you define the `pct-non-white` variable, and who is included.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pct-non-white variable\n",
    "df_modeling[\"pct-non-white\"] = ...  ## YOUR CODE HERE\n",
    "\n",
    "# plot correlations to feature variables\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:green\">Takeaways</span>** \n",
    "\n",
    "- <span style=\"color:green\">... your thoughts here ...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Another strategy is to fit a model that predicts the percent of a given race based on feature variables. If that model performs well, we know that our model predicting evictions could also make accurate inferences about racial breakdowns within tracts.</span>\n",
    "\n",
    "<span style=\"color:green\">**Below, train a model that predicts `pct-white` based on the same `FEATURE_VARS` used to train our eviction model earlier. Then assess how well the model performs, and write up a few takeaways about what that means for race proxy variables in our eviction model.** Remember, you can reuse code from earlier steps.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X_train, y_train, X_test, and y_test\n",
    "# we can use the same split as before\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model to predict pct-white\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate simple performance metrics (R2, MSE, MAE)\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for any other exploration of model performance you'd like to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:green\">Takeaways</span>** \n",
    "\n",
    "<span style=\"color:green\">... add your thoughts here ...</span>\n",
    "\n",
    "\n",
    "<span style=\"color:green\">**End of activity, wait for group to reconvene and discuss**</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">*D.2 DISCUSSION*</span>\n",
    "\n",
    "<span style=\"color:green\">What are some approaches we can use to answer this question? Think about things like:</span> \n",
    "\n",
    "- <span style=\"color:green\">How does the format of the data about race impact our strategy?</span> \n",
    "\n",
    "- <span style=\"color:green\">What model performance metrics do we want to consider?</span> \n",
    "\n",
    "- <span style=\"color:green\">What visuals do we want to produce?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### <span style=\"color:green\">*D.2 ACTIVITY*</span>\n",
    "\n",
    "> Work independently for 15-20 minutes. Start here and stop where \"end of activity\" is indicated\n",
    "\n",
    "<span style=\"color:green\">Calculate the correlation between each of the race variables in the model with error and absolute error. Remember that you can re-use code from previous sections.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns to df_test for error and absolute error\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "# calculate correlation\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "# plot correlation heatmap\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:green\">Takeaways</span>** \n",
    "\n",
    "- <span style=\"color:green\">... add your thoughts here ...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Generate at least one other visual that helps to compare error rates between different racial groups. You could also explore another method of determining whether error is dependent on race percentages, such as fitting another model.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**End of activity, wait for group to reconvene and discuss**</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into explainability in detail in this exercise. For quick reference, let's look at the [feature importances](https://www.aporia.com/learn/feature-importance/feature-importance-7-methods-and-a-quick-tutorial/#:~:text=In%20machine%20learning%2C%20feature%20importance,linear%20models%2C%20and%20neural%20networks.) of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.named_steps[\"model\"].feature_importances_\n",
    "feature_importance = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"features\": FEATURE_VARS,\n",
    "        \"importance\": model_pipeline.named_steps[\"model\"].feature_importances_,\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    feature_importance.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **E.1 Monitoring and evaluation:** How are we planning to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">*E.1-2 DISCUSSION*</span>\n",
    "\n",
    "\n",
    "<span style=\"color:green\">*E.1 and E.2:* Think about how you could monitor/evaluate the model moving forward, and put steps for redress in place.</span>\n",
    "\n",
    "- <span style=\"color:green\">What are some possible real-world consequences of the model performing poorly / making mistakes? What is the potential harm or inequity from incorrect model estimates?</span>\n",
    "\n",
    "- <span style=\"color:green\">How might you determine what the performance cutoff is for the model being good enough to use in practice?</span>\n",
    "\n",
    "- <span style=\"color:green\">What metric could you use for the above? What are some of the pros and cons of different matrics? Think about the consequences of false positives vs. false negatives in practice. Is one less desirable than the other, and how can that be reflected in your metric?</span>\n",
    "\n",
    "- <span style=\"color:green\">If/when the model is deployed in practice, will there be any human review of the model's decisions? In which cases will there be human review, and how will that be integrated?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Science Ethics Checklist generated with [deon](http://deon.drivendata.org).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
